{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b108a696-a0f7-4c6a-86ac-39d8ad94d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This is a paragraph\n",
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    " \n",
    "text = \"<p>    This is a paragraph</p>\" \n",
    " \n",
    "result = re.sub(r'<.?p>', '', text)\n",
    " \n",
    "print(result) \n",
    "#    This is a paragraph\n",
    "\n",
    "import re\n",
    "\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "\n",
    "headline_no_tag = re.sub(r'<.?h1>', '', headline_one)\n",
    "\n",
    "print(headline_no_tag)\n",
    "\n",
    "tweet_no_at = re.sub(r'@', '', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30faed19-4e13-4a7a-9808-51993548b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n",
      "Stopwords type: <class 'set'>\n",
      "Words Tokenized: ['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n",
      "Text without Stops: ['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "nbc_statement = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
    " \n",
    "word_tokens = word_tokenize(nbc_statement) \n",
    "# tokenize nbc_statement\n",
    " \n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
    " \n",
    "print(statement_no_stop)\n",
    "# ['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "\n",
    "try:\n",
    "  print(f'Stopwords type: {type(stop_words)}')\n",
    "except:\n",
    "  print('Expected a variable called `stop_words`')\n",
    "try:\n",
    "  print(f'Words Tokenized: {tokenized_survey}')\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_survey`')\n",
    "try:\n",
    "  print(f'Text without Stops: {text_no_stops}')\n",
    "except:\n",
    "  print('Expected a variable called `text_no_stops`')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58b8883-05bc-423a-ac00-fc5d0b026542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'text']\n",
      "['Tokenize', 'this', 'text']\n",
      "['Tokenize this sentence.', 'Also, tokenize this sentence.']\n",
      "Word Tokenization:\n",
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n",
      "Sentence Tokenization:\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    " \n",
    "print(tokenized)\n",
    "# [\"Tokenize\", \"this\", \"text\"]\n",
    "\n",
    "text_split = text.split()\n",
    "print(text_split)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    " \n",
    "print(tokenized)\n",
    "# ['Tokenize this sentence.', 'Also, tokenize this sentence.']\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "\n",
    "try:\n",
    "  print('Word Tokenization:')\n",
    "  print(tokenized_by_word)\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_by_word`')\n",
    "try:\n",
    "  print('Sentence Tokenization:')\n",
    "  print(tokenized_by_sentence)\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_by_sentence`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1bef4e1-5afc-4bdb-8dc3-a069b3504865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A stemmer exists:\n",
      "<PorterStemmer>\n",
      "Words Tokenized:\n",
      "['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Stemmed Words:\n",
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'it', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  print('A stemmer exists:')\n",
    "  print(stemmer)\n",
    "except:\n",
    "  print('Expected a variable called `stemmer`')\n",
    "try:\n",
    "  print('Words Tokenized:')\n",
    "  print(island_tokenized)\n",
    "except:\n",
    "  print('Expected a variable called `island_tokenized`')\n",
    "try:\n",
    "  print('Stemmed Words:')\n",
    "  print(stemmed)\n",
    "except:\n",
    "  print('Expected a variable called `stemmed`')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd0c8bb-8eef-411d-a664-30aa4613aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lemmatizer exists: <WordNetLemmatizer>\n",
      "Words Tokenized: ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Lemmatized Words: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "try:\n",
    "  print(f'A lemmatizer exists: {lemmatizer}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatizer`')\n",
    "try:\n",
    "  print(f'Words Tokenized: {tokenized_string}')\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_string`')\n",
    "try:\n",
    "  print(f'Lemmatized Words: {lemmatized_words}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatized_words`')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9d552e-2501-4b40-a906-717583c7be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'old', 'be', 'the', 'country', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "tokenized = [\"How\", \"old\", \"is\", \"the\", \"country\", \"Indonesia\"]\n",
    " \n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    " \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03ca107-5adf-416e-ae31-44613b197c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working', 'in', 'local', 'medium', ',', 'she', 'wa', 'both', 'the', 'youngest', 'news', 'anchor', 'and', 'the', 'first', 'black', 'female', 'news', 'anchor', 'at', 'nashville', \"'s\", 'wlac-tv']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from part_of_speech import get_part_of_speech\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "oprah_wiki = '<p>Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville\\'s WLAC-TV. </p>'\n",
    "\n",
    "oprah_no_p = re.sub(r'<.?p>', '', oprah_wiki)\n",
    "\n",
    "oprah_no_period = re.sub(r'\\.', '', oprah_no_p)\n",
    "\n",
    "oprah_lower_case = oprah_no_period.lower()\n",
    "\n",
    "tokenized = word_tokenize(oprah_lower_case)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7d0d6-162f-46c6-8f90-2c3e6a1c1f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
